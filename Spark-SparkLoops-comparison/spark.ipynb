{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from itertools import combinations\n",
    "import time\n",
    "\n",
    "# Stop the existing SparkContext (if it exists)\n",
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    pass\n",
    "# create a SparkContext\n",
    "sc = SparkContext(appName=\"spark-task\")\n",
    "# load the page view statistics data into an RDD\n",
    "data_file = \"pagecounts-20160101-000000_parsed.out\"\n",
    "lines_rdd = sc.textFile(data_file)\n",
    "# Extract the relevant attributes from each line and store as a tuple and split index 1 by _\n",
    "data_rdd = lines_rdd.map(lambda line: tuple(line.split()))\n",
    "# data format is : projectCode pageTitle pageHits pageSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 1 -> Get min, max, and average of page size\n",
    "\n",
    "# start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Calculate the minimum, maximum, and average page size\n",
    "min_size = data_rdd.filter(lambda data: len(data) == 4 and data[3].isdigit()).map(lambda data: int(data[3])).reduce(lambda x, y: min(x, y))\n",
    "max_size = data_rdd.filter(lambda data: len(data) == 4 and data[3].isdigit()).map(lambda data: int(data[3])).reduce(lambda x, y: max(x, y))\n",
    "\n",
    "sum_size = data_rdd.filter(lambda data: len(data) == 4 and data[3].isdigit()).map(lambda data: int(data[3])).reduce(lambda x, y: x + y)\n",
    "count_size = data_rdd.filter(lambda data: len(data) == 4 and data[3].isdigit()).count()\n",
    "avg_size = sum_size / count_size\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Write the results to a file\n",
    "with open(\"spark-MapReduce.txt\", \"w\",encoding=\"utf-8\") as f:\n",
    "    f.write(\"Spark Map Reduce \\n\")\n",
    "    f.write(\"---------------------- Problem 1 ------------------ \\n\")\n",
    "    elapsed_time = \"Elapsed time:\"+str(elapsed_time)+\" seconds\\n\"\n",
    "    f.write(elapsed_time)\n",
    "    f.write(\"Min page size: {}\\n\".format(min_size))\n",
    "    f.write(\"Max page size: {}\\n\".format(max_size))\n",
    "    f.write(\"Avg page size: {}\\n\".format(avg_size))\n",
    "    f.write(\"______________________________________________________\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 2 -> Get the number of page titles that start with \"The\" and are not part of the English project\n",
    "\n",
    "# start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Determine the number of page titles that start with \"The\" and are not part of the English project\n",
    "num_non_en_the_pages = data_rdd.filter(lambda data: data[0] != \"en\" and data[1].startswith(\"The_\")).map(lambda data: (True, 1)).reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "elapsed_time = \"Elapsed time: {} seconds\".format(elapsed_time)\n",
    "\n",
    "# Write the results to a file\n",
    "with open(\"spark-MapReduce.txt\", \"a\",encoding=\"utf-8\") as f:\n",
    "    f.write(\"---------------------- Problem 2 ------------------ \\n\")\n",
    "    elapsed_time = \"Elapsed time:\"+str(elapsed_time)+\"\\n\"\n",
    "    f.write(elapsed_time)\n",
    "    f.write(\"Number of 'The' page titles that are not part of the English project: {}\\n\".format(num_non_en_the_pages.first()[1]))\n",
    "    f.write(\"______________________________________________________\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 3 -> Get the number of unique terms in the page titles\n",
    "\n",
    "# start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "terms_rdd = data_rdd.flatMap(lambda data: data[1].split(\"_\"))  \n",
    "\n",
    "# Map each term to a key-value pair with value 1\n",
    "term_map_rdd = terms_rdd.map(lambda term: (term, 1))\n",
    "\n",
    "# Reduce by key to get the count of each term\n",
    "term_count_rdd = term_map_rdd.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Count the number of unique terms\n",
    "unique_terms = term_count_rdd.count()\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "elapsed_time = \"Elapsed time: {} seconds\".format(elapsed_time)\n",
    "\n",
    "# Write the results to a file\n",
    "with open(\"spark-MapReduce.txt\", \"a\",encoding=\"utf-8\") as f:\n",
    "    f.write(\"---------------------- Problem 3 ------------------ \\n\")\n",
    "    elapsed_time = \"Elapsed time:\"+str(elapsed_time)+\"\\n\"\n",
    "    f.write(elapsed_time)\n",
    "    f.write(\"Number of unique terms in the page titles : {}\\n\".format(unique_terms))\n",
    "    f.write(\"______________________________________________________\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 4 -> Get the number of occurrences of each page title\n",
    "\n",
    "# start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Extract the page titles\n",
    "titles_rdd = data_rdd.map(lambda data: data[1])\n",
    "\n",
    "# Count the number of occurrences of each title\n",
    "title_count_rdd = titles_rdd.map(lambda title: (title, 1)).reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# # Print each row of the whole result without using collect \n",
    "# for title, count in title_count_rdd.take(100):\n",
    "#     print(title, count)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "elapsed_time = \"Elapsed time: {} seconds\".format(elapsed_time)\n",
    "\n",
    "# Write the results to a file\n",
    "with open(\"spark-MapReduce.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"---------------------- Problem 4 ------------------ \\n\")\n",
    "    elapsed_time = \"Elapsed time:\"+str(elapsed_time)+\"\\n\"\n",
    "    f.write(elapsed_time)\n",
    "    for title, count in title_count_rdd.collect():\n",
    "        f.write(\"{}: {}\\n\".format(title, count))\n",
    "    f.write(\"______________________________________________________\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 5 -> Get pairs of pages with the same title and combination of pairs\n",
    "\n",
    "# start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Group pages with the same title together\n",
    "grouped_pages_rdd = data_rdd.groupBy(lambda data: data[1])\n",
    "\n",
    "# For each group of pages with the same title, save the data for each page in a pairwise manner\n",
    "page_pairs_rdd = grouped_pages_rdd.flatMap(lambda x: [(x[0], pair) for pair in combinations(x[1], 2)])\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "elapsed_time = \"Elapsed time: {} seconds\".format(elapsed_time)\n",
    "\n",
    "# Write the results to a file\n",
    "with open(\"spark-MapReduce.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"---------------------- Problem 5 ------------------ \\n\")\n",
    "    elapsed_time = \"Elapsed time:\"+str(elapsed_time)+\"\\n\"\n",
    "    f.write(elapsed_time)\n",
    "    \n",
    "    for title, pair in page_pairs_rdd.toLocalIterator():\n",
    "        f.write(\"Pages with title '{}':\".format(title))\n",
    "        f.write(\"\\n\")\n",
    "        for data in pair:\n",
    "            f.write(\"\\t{}\\n\".format(data))\n",
    "    f.write(\"______________________________________________________\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
