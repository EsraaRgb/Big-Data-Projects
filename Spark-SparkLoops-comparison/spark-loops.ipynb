{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext,SparkConf\n",
    "import time\n",
    "\n",
    "# Stop the existing SparkContext (if it exists)\n",
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    pass\n",
    "# create a SparkContext\n",
    "# sc = SparkContext(appName=\"ComputePageSizeStats\")\n",
    "conf = SparkConf().setAppName(\"PageCounts\")\n",
    "sc = SparkContext(conf=conf)\n",
    "# load the page view statistics data into an RDD\n",
    "rdd = sc.textFile(\"pagecounts-20160101-000000_parsed.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 1 -> Get min, max, and average of page size\n",
    "\n",
    "# start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# initialize variables\n",
    "min_size = float('inf')\n",
    "max_size = float('-inf')\n",
    "total_size = 0\n",
    "count = 0\n",
    "\n",
    "# loop through each line in the RDD\n",
    "for line in rdd.toLocalIterator():\n",
    "    # parse the line and extract the page size\n",
    "    fields = line.split()\n",
    "    if len(fields) >= 4:\n",
    "        try:\n",
    "            size = int(fields[3])\n",
    "            if size >= 0:\n",
    "                # update min, max, and total size\n",
    "                min_size = min(min_size, size)\n",
    "                max_size = max(max_size, size)\n",
    "                total_size += size\n",
    "                count += 1\n",
    "        except ValueError:\n",
    "            # skip lines with invalid page size\n",
    "            pass\n",
    "\n",
    "# compute the average page size\n",
    "if count > 0:\n",
    "    avg_size = total_size / count\n",
    "else:\n",
    "    avg_size = 0\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "\n",
    "with open(\"sparkLoops.txt\", \"w\",encoding=\"utf-8\") as f:\n",
    "    f.write(\"Spark Loops\\n\")\n",
    "    f.write(\"---------------------- Problem 1 ------------------ \\n\")\n",
    "    elapsed_time = \"Elapsed time:\"+str(elapsed_time)+\" seconds \\n\"\n",
    "    f.write(elapsed_time)\n",
    "    f.write(\"Min page size: {}\\n\".format(min_size))\n",
    "    f.write(\"Max page size: {}\\n\".format(max_size))\n",
    "    f.write(\"Avg page size: {}\\n\".format(avg_size))\n",
    "    f.write(\"______________________________________________________\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 2 -> Get the number of pages that start with \"The\" and are not part of the English project\n",
    "\n",
    "# start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# initialize variables\n",
    "num_the_pages = 0\n",
    "num_non_en_pages = 0\n",
    "\n",
    "# loop through each line in the RDD\n",
    "for line in rdd.toLocalIterator():\n",
    "    # split the line into fields\n",
    "    fields = line.split()\n",
    "    \n",
    "    # check if the page title starts with \"The\"\n",
    "    if fields[1].startswith(\"The_\"):\n",
    "        num_the_pages += 1\n",
    "        \n",
    "        # check if the page is not part of the English project\n",
    "        if fields[0] != \"en\":\n",
    "            num_non_en_pages += 1\n",
    "\n",
    "# end the timer\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "with open(\"sparkLoops.txt\", \"a\",encoding=\"utf-8\") as f:\n",
    "    f.write(\"---------------------- Problem 2 ------------------ \\n\")\n",
    "    elapsed_time = \"Elapsed time:\"+str(elapsed_time)+\" seconds \\n\"\n",
    "    f.write(elapsed_time)\n",
    "    f.write(\"Number of non-English pages starting with 'The': {}\\n\".format(num_non_en_pages))\n",
    "    f.write(\"______________________________________________________\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 3 -> Get the number of unique terms in the page titles\n",
    "\n",
    "# start the timer\n",
    "start_time = time.time()\n",
    "# initialize variables\n",
    "unique_terms = set()\n",
    "\n",
    "# loop through each line in the RDD\n",
    "for line in rdd.toLocalIterator():\n",
    "    # extract the page title and split it into terms\n",
    "    title = line.split()[1]\n",
    "    terms = title.split('_')\n",
    "    \n",
    "    # add each term to the set of unique terms\n",
    "    for term in terms:\n",
    "        unique_terms.add(term)\n",
    "\n",
    "# count the number of unique terms\n",
    "num_terms = len(unique_terms)\n",
    "\n",
    "# end the timer\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "with open(\"sparkLoops.txt\", \"a\",encoding=\"utf-8\") as f:\n",
    "    f.write(\"---------------------- Problem 3 ------------------ \\n\")\n",
    "    elapsed_time = \"Elapsed time:\"+str(elapsed_time)+\" seconds \\n\"\n",
    "    f.write(elapsed_time)\n",
    "    f.write(\"Number of unique  terms in the page titles :{}\\n\".format(num_terms))\n",
    "    f.write(\"______________________________________________________\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 4 -> Get how many times a certain title appears in the dataset\n",
    "\n",
    "# Stop the existing SparkContext (if it exists)\n",
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Configure and initialize Spark\n",
    "conf = SparkConf().setAppName(\"PageStatistics\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Create a SparkSession\n",
    "page_views = sc.textFile(\"pagecounts-20160101-000000_parsed.out\")\n",
    "\n",
    "title_counts = {}\n",
    "# Loop over each line in the page view statistics\n",
    "for line in page_views.toLocalIterator():\n",
    "    # Split the line into fields\n",
    "    fields = line.split(\" \")\n",
    "    # Extract the title for the current page\n",
    "    title = fields[1]\n",
    "    views = 1\n",
    "    # If the title is not in the dictionary, set views to 1\n",
    "    if title not in title_counts:\n",
    "        views = 1\n",
    "    # If the title is already in the dictionary, increase views by 1\n",
    "    else:\n",
    "        views = title_counts[title] + 1\n",
    "    # Update the page views for the current title\n",
    "    title_counts[title] = views\n",
    "\n",
    "# end the timer\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Write the title counts to a file\n",
    "with open(\"sparkLoops.txt\", \"a\",encoding=\"utf-8\") as f:\n",
    "        f.write(\"---------------------- Problem 4 ------------------ \\n\")\n",
    "        elapsed_time = \"Elapsed time:\"+str(elapsed_time)+\" seconds \\n\"\n",
    "        f.write(elapsed_time)\n",
    "        f.write(\"Number of times a certain title appears in the dataset :\\n\")\n",
    "        for title, count in title_counts.items():\n",
    "            f.write(\"{}: {}\\n\".format(title, count))\n",
    "        f.write(\"______________________________________________________\\n\")\n",
    "# Stop the SparkSession\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 5 -> Get pairs of pages with the same title and combination of pairs\n",
    "\n",
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "conf = SparkConf().setAppName(\"PageCounts\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "lines = sc.textFile(\"pagecounts-20160101-000000_parsed.out\")\n",
    "parsed_data = []\n",
    "\n",
    "# Using a loop to split each line into fields\n",
    "for line in lines.toLocalIterator():\n",
    "    fields = line.split(\" \")\n",
    "    if len(fields) >= 4:\n",
    "        parsed_data.append(fields)\n",
    "\n",
    "converted_data = []\n",
    "\n",
    "# Using a loop to convert the data types and extract relevant fields\n",
    "for fields in parsed_data:\n",
    "    converted_data.append((fields[0], fields[1], int(fields[2]), int(fields[3])))\n",
    "\n",
    "# Using a dictionary to combine the data by key\n",
    "combined_data_dict = {}\n",
    "for data in converted_data:\n",
    "    key = data[1]\n",
    "    if key in combined_data_dict:\n",
    "        combined_data_dict[key].append((data[0],data[1], data[2], data[3]))\n",
    "    else:\n",
    "        combined_data_dict[key] = [(data[0],data[1], data[2], data[3])]\n",
    "\n",
    "# # Using a loop to format the output data\n",
    "# output_data = []\n",
    "# for key, value in combined_data_dict.items():\n",
    "#     output_data.append(\"Title: {}\\n\".format(key))\n",
    "#     for item in value:\n",
    "#         if len(item) >= 3:\n",
    "#             output_data.append(\"{{{}}}\\n\".format(\"\\t\".join(map(str, item))))\n",
    "# Using a loop to format the output data\n",
    "output_data = []\n",
    "for key, value in combined_data_dict.items():\n",
    "    output_data.append(\"Title: {}\\n\".format(key))\n",
    "    for i, item in enumerate(value):\n",
    "        if len(item) >= 3:\n",
    "            for j in range(i+1, len(value)):\n",
    "                if len(value[j]) >= 3:\n",
    "                    pair = [item, value[j]]\n",
    "                    output_data.append(\"{{{}}}\\n\".format(\"\\t\".join(map(str, pair))))\n",
    "\n",
    "# end the timer\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "with open(\"sparkLoops.txt\", \"a\",encoding=\"utf-8\") as f:\n",
    "    f.write(\"---------------------- Problem 5 ------------------ \\n\")\n",
    "    elapsed_time = \"Elapsed time:\"+str(elapsed_time)+\" seconds \\n\"\n",
    "    f.write(elapsed_time)   \n",
    "    f.writelines(output_data)\n",
    "\n",
    "# Stop the Spark context to release resources\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
